# Ollama LLM Configuration (for RAG service)
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_MODEL=llama3.1:8b

# Alternative: vLLM Configuration
# VLLM_BASE_URL=http://localhost:8001/v1
# VLLM_MODEL=meta-llama/Llama-3.2-3B-Instruct

# Pinecone Configuration
PINECONE_HOST=localhost
PINECONE_PORT=5081
PINECONE_API_KEY=pclocal
PINECONE_ENVIRONMENT=local

# Embeddings Configuration
EMBEDDINGS_PROVIDER=local
SENTENCE_TRANSFORMER_URL=http://localhost:8000
MODEL_NAME=all-MiniLM-L6-v2

# Optional: NVIDIA API (for embeddings or reranking)
# NVIDIA_API_KEY=your_nvidia_api_key_here

# Optional: Cohere (for reranking)
# COHERE_API_KEY=your_cohere_api_key_here
